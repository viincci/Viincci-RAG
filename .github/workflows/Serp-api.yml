name: SerpAPI Integration Tests

on:
  workflow_dispatch:
    inputs:
      test_domain:
        description: 'Domain to test (botany, medical, mathematics, carpentry)'
        required: false
        default: 'botany'
      test_query:
        description: 'Query to test (leave empty for default)'
        required: false
        default: ''
  schedule:
    # Run weekly on Sundays at 2 AM UTC to monitor API health
    - cron: '0 2 * * 0'

jobs:
  test-serpapi-credits:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt

    - name: Create directory structure
      run: |
        mkdir -p V4/config
        mkdir -p V4/db
        mkdir -p _posts

    - name: Check SerpAPI Credits
      env:
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      run: |
        python -c "
        import os
        from V4.ApiMonitor import SerpAPIMonitor
        from V4.ConfigManager import ConfigManager
        
        if not os.getenv('SERP_API_KEY'):
            print('‚ùå SERP_API_KEY not set in secrets')
            exit(1)
        
        config = ConfigManager(verbose=True)
        monitor = SerpAPIMonitor(config)
        
        print('\nüîç Checking SerpAPI Account Status...\n')
        status = monitor.check_credits(verbose=True)
        
        # Save status for later jobs
        with open('api_status.txt', 'w') as f:
            f.write(f'{status[\"searches_remaining\"]}')
        
        if not status['can_proceed']:
            print('‚ùå Insufficient API credits')
            exit(1)
        
        print('‚úÖ API credits OK')
        "

    - name: Upload API status
      uses: actions/upload-artifact@v4
      with:
        name: api-status
        path: api_status.txt

  test-serpapi-search:
    runs-on: ubuntu-latest
    needs: test-serpapi-credits
    if: success()
    strategy:
      matrix:
        domain: 
          - botany
          - medical
          - mathematics
      fail-fast: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directory structure
      run: |
        mkdir -p V4/config
        mkdir -p V4/db
        mkdir -p _posts

    - name: Download API status
      uses: actions/download-artifact@v4
      with:
        name: api-status

    - name: Test SerpAPI Search - ${{ matrix.domain }}
      env:
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      run: |
        python -c "
        import os
        from V4.ConfigManager import ConfigManager
        from V4.Spider import UniversalResearchSpider
        
        # Test queries per domain
        test_queries = {
            'botany': 'Aloe vera',
            'medical': 'diabetes',
            'mathematics': 'Pythagorean theorem',
            'carpentry': 'dovetail joint'
        }
        
        domain = '${{ matrix.domain }}'
        query = test_queries.get(domain, 'test')
        
        print(f'\nüîç Testing SerpAPI Search')
        print(f'Domain: {domain}')
        print(f'Query: {query}\n')
        
        config = ConfigManager(domain=domain, verbose=False)
        spider = UniversalResearchSpider(config, check_credits=True)
        
        # Perform limited search (max 5 sources to conserve credits)
        config._configs['search_config']['search']['max_sources'] = 5
        
        try:
            results = spider.search_serpapi(query)
            
            if len(results) == 0:
                print('‚ùå No results returned from SerpAPI')
                exit(1)
            
            print(f'‚úÖ Found {len(results)} results')
            print(f'\nSample results:')
            for i, result in enumerate(results[:3], 1):
                print(f'{i}. {result[\"title\"][:60]}...')
                print(f'   URL: {result[\"url\"][:80]}')
            
            # Save results
            import json
            with open(f'{domain}_search_results.json', 'w') as f:
                json.dump(results[:5], f, indent=2)
            
            print(f'\n‚úÖ Search test passed for {domain}')
            
        except Exception as e:
            print(f'‚ùå Search failed: {str(e)}')
            exit(1)
        "

    - name: Upload search results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: search-results-${{ matrix.domain }}
        path: ${{ matrix.domain }}_search_results.json

  test-serpapi-extraction:
    runs-on: ubuntu-latest
    needs: test-serpapi-search
    if: success()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directory structure
      run: |
        mkdir -p V4/config
        mkdir -p V4/db
        mkdir -p _posts

    - name: Download search results
      uses: actions/download-artifact@v4
      with:
        name: search-results-botany

    - name: Test Content Extraction
      env:
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      run: |
        python -c "
        import json
        from V4.ConfigManager import ConfigManager
        from V4.Spider import UniversalResearchSpider
        
        print('\nüîç Testing Content Extraction\n')
        
        # Load search results
        with open('botany_search_results.json', 'r') as f:
            results = json.load(f)
        
        if not results:
            print('‚ùå No search results to test')
            exit(1)
        
        config = ConfigManager(domain='botany', verbose=False)
        spider = UniversalResearchSpider(config, check_credits=False)
        
        # Test extraction on first result
        test_url = results[0]['url']
        doc_type = results[0].get('doc_type', 'html')
        
        print(f'Testing extraction from: {test_url}')
        print(f'Document type: {doc_type}\n')
        
        try:
            content = spider.extract_content(test_url, doc_type)
            
            if not content:
                print('‚ö†Ô∏è  No content extracted (may be blocked or empty)')
                exit(0)  # Don't fail - some sites may block scrapers
            
            text_length = len(content.get('text', ''))
            
            print(f'‚úÖ Content extracted successfully')
            print(f'Text length: {text_length} characters')
            print(f'Source: {content[\"metadata\"][\"source\"]}')
            print(f'Reliability: {content[\"metadata\"][\"reliability\"]}')
            
            if text_length < 100:
                print('‚ö†Ô∏è  Warning: Extracted text is very short')
            
        except Exception as e:
            print(f'‚ö†Ô∏è  Extraction failed: {str(e)}')
            print('This may be expected for some sites')
            exit(0)  # Don't fail the job
        "

  test-full-research-pipeline:
    runs-on: ubuntu-latest
    needs: test-serpapi-credits
    if: github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directory structure
      run: |
        mkdir -p V4/config
        mkdir -p V4/db
        mkdir -p _posts

    - name: Run Full Research Pipeline (Limited)
      env:
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      run: |
        python -c "
        import os
        from V4.ConfigManager import ConfigManager
        from V4.Spider import UniversalResearchSpider
        from V4.ApiMonitor import SerpAPIMonitor
        
        # Get inputs or use defaults
        domain = '${{ github.event.inputs.test_domain }}' or 'botany'
        query = '${{ github.event.inputs.test_query }}' or 'Rosa rubiginosa'
        
        print(f'\nüî¨ Full Research Pipeline Test')
        print(f'Domain: {domain}')
        print(f'Query: {query}\n')
        
        # Initialize
        config = ConfigManager(domain=domain, verbose=True)
        monitor = SerpAPIMonitor(config)
        
        # Check credits first
        print('Checking API credits...')
        status = monitor.check_credits(verbose=True)
        
        if not status['can_proceed']:
            print('‚ùå Insufficient credits')
            exit(1)
        
        # Estimate cost
        estimate = monitor.estimate_research_cost(query, questions=2)
        monitor.print_estimate(estimate)
        
        if not estimate['can_afford']:
            print('‚ùå Cannot afford this research')
            exit(1)
        
        # Limit sources to conserve credits
        config._configs['search_config']['search']['max_sources'] = 10
        
        # Perform research
        spider = UniversalResearchSpider(config, check_credits=True)
        
        try:
            sources = spider.research(query, estimate_first=False)
            
            if not sources:
                print('‚ùå No sources collected')
                exit(1)
            
            print(f'\n‚úÖ Research completed successfully')
            print(f'Total sources: {len(sources)}')
            
            # Print sample sources
            print(f'\nTop 3 sources:')
            for i, source in enumerate(sources[:3], 1):
                print(f'{i}. {source[\"metadata\"][\"source\"]}')
                print(f'   Reliability: {source[\"metadata\"][\"reliability\"]}')
                print(f'   Text length: {len(source[\"text\"])} chars')
            
        except Exception as e:
            print(f'‚ùå Research failed: {str(e)}')
            import traceback
            traceback.print_exc()
            exit(1)
        "

    - name: Check API credits after test
      if: always()
      env:
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      run: |
        python -c "
        from V4.ApiMonitor import SerpAPIMonitor
        from V4.ConfigManager import ConfigManager
        
        config = ConfigManager(verbose=False)
        monitor = SerpAPIMonitor(config)
        
        print('\nüìä Final API Status:')
        status = monitor.check_credits(verbose=True)
        "

  summary:
    runs-on: ubuntu-latest
    needs: [test-serpapi-credits, test-serpapi-search, test-serpapi-extraction]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: '*'
        path: artifacts/

    - name: Generate Test Report
      run: |
        echo "# SerpAPI Integration Test Report" > report.md
        echo "" >> report.md
        echo "**Date:** $(date)" >> report.md
        echo "" >> report.md
        
        echo "## Test Results" >> report.md
        echo "" >> report.md
        
        if [ -f artifacts/api-status/api_status.txt ]; then
          credits=$(cat artifacts/api-status/api_status.txt)
          echo "- **API Credits Remaining:** $credits" >> report.md
        fi
        
        echo "- **Credits Check:** ${{ needs.test-serpapi-credits.result }}" >> report.md
        echo "- **Search Tests:** ${{ needs.test-serpapi-search.result }}" >> report.md
        echo "- **Extraction Tests:** ${{ needs.test-serpapi-extraction.result }}" >> report.md
        echo "" >> report.md
        
        echo "## Search Results by Domain" >> report.md
        for domain in botany medical mathematics; do
          if [ -f "artifacts/search-results-${domain}/${domain}_search_results.json" ]; then
            count=$(cat "artifacts/search-results-${domain}/${domain}_search_results.json" | grep -o '"url"' | wc -l)
            echo "- **${domain}:** ${count} results" >> report.md
          fi
        done
        
        echo "" >> report.md
        echo "---" >> report.md
        echo "Generated by GitHub Actions" >> report.md
        
        cat report.md

    - name: Upload Test Report
      uses: actions/upload-artifact@v4
      with:
        name: test-report
        path: report.md

    - name: Check Overall Status
      run: |
        echo ""
        echo "========================================="
        echo "SerpAPI Integration Test Summary"
        echo "========================================="
        echo ""
        echo "Credits Check: ${{ needs.test-serpapi-credits.result }}"
        echo "Search Tests: ${{ needs.test-serpapi-search.result }}"
        echo "Extraction Tests: ${{ needs.test-serpapi-extraction.result }}"
        echo ""
        
        if [ "${{ needs.test-serpapi-credits.result }}" = "success" ] && \
           [ "${{ needs.test-serpapi-search.result }}" = "success" ]; then
          echo "‚úÖ OVERALL STATUS: PASSED"
          echo "========================================="
          exit 0
        else
          echo "‚ùå OVERALL STATUS: FAILED"
          echo "========================================="
          exit 1
        fi
